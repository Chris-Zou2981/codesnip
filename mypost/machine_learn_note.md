# numpy

-  声明数组：`np.array([[1, 2], [3, 4]])`
-  形状：`arr.sharp` = `(2, 2)`
-  类型：`arr.dtype` = `int64`
-  形状相同：`[1, 2] * [3, 4]` = `[3, 8]`
-  形状不同：`[[1, 2], [3, 4]] * [10, 20]` = `[[10, 40], [30, 80]]`
-  数组和数值：`[1, 2] * 10`
-  访问：`arr[[1, 3, 5]]`，`arr[arr < 5]`

---

# matplotlib

```
x = [1, 3, 5]
y = [2, 4, 6]
plt.plot(x, y, label='test')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Hello')
plt.legend()
plt.show()
```

---

# 感知机

概念：接受多个信号，输出一个信号
- 输入信号：x1, x2
- 权重：w1, w2，相当于电阻，控制信号流动的难度
- 输出信号：y
- 阈值：theta，超过阈值神经元被激活，y 为 1

```
w1*x1 + w2*x2 <= theta， y = 0
w1*x1 + w2*x2 > theta, y = 1
```

---
# 逻辑电路：线性可分

- 与门：x1, x2 同时为 1 时 y 为 1
	- w1 = 0.5, w2 = 0.5, theta = 0.7
- 与非门：x1, x2 同时为 1 时 y 为 0
	- w1 = -0.5, w2 = -0.5, theta = -0.7
- 或门：x1, x2 只要有一个为 1 y 就为 1
	- w1 = 1, w2 = 1, theta = 0

三种电路构造形式一样，只有`权重`和`阈值`不同，把构造式子修改下阈值就相当于 `偏置` b

```
b + w1*x1 + w2*x2 <= 0， y = 0
b + w1*x1 + w2*x2 > 0, y = 1
```

这里 `b = -theta`

---
# 与门代码实现

```
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
    return 0
    else:
    return 1
``` 

权重是控制输入信号的重要性，偏置调整神经元被激活的容易程度。

---
# 非线性的异或门，多层感知机

x1 和 x2 只有一个为 1 时 y 为 1，感知机不能表示异或门，因为它是非线性可分的，所以需要多层感知机

```
def XOR(x1, x2):
   s1 = NAND(x1, x2)
   s2 = OR(x1, x2)
   y = AND(s1, s2)
   return y
```

单层感知机只能表示线性空间，而多层感知机可以表示非线性空间

---
# 神经网络，激活函数，阶跃函数

- 输入层，输出层和中间层
- 激活函数：将输入信号的总和转换为输出信号的函数	
- 一旦输入超过阈值，就切换输出，这样的函数称为 `阶跃函数`
- 感知机中使用了阶跃函数作为激活函数，总和大于 0 时返回 1，否则返回 0

---

# sigmoid 函数

```
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

- sigmoid 函数是一条 0 到 1 平滑曲线，阶跃函数是一条 0 到 1 像阶梯一样的折线，两者都属于非线性的函数。
- 神经网络用的 sigmoid 平滑变化，感知机的节约函数急剧变化
- 神经网络的激活函数必须使用非线性函数，因为使用线性函数的话，加深神经网络的层数就没有意义了

---

# ReLu 函数

ReLU 函数在输入大于 0 时，直接输出该值；在输入小于等于 0 时，输出 0。

```
def relu(x):
    return np.maximum(0, x)
```

---

# 矩阵乘法

- `np.dot`
- 矩阵 A 乘 矩阵 B，A 的第 2 维和 B 的第 1 维的元素个数要一致，结果的形状是 A 的第一维和 B 的第二维
	- 2 * 3 的矩阵 A 乘以 3 * 4 的矩阵 B，结果的形状是 2 * 4
- 二维矩阵和一维数组相乘时，也要保持对应维度的元素个数一致, 结果形状是 A 的第一维长度
	- 2 * 3 的矩阵 A 乘以 3 个元素的数组 a，结果是一个 2 个元素的数组
	- 2 个元素的数组 X 乘以 2 * 3 的矩阵 W 时，结果是一个 3 个元素的数组

---

# 使用矩阵实现 神经网络


- 第一步：`A = X * W + B`
	- X 是输入神经元：`[x1, x2]`
	- W 是权重：`[[w11, w21, w31], [w12, , w22, w32]]`
	- B 是偏置：`[b1, b2, b3]`
	- A 是下一层：`[a1, a2, a3]`
- 第二步：`Z = sigmoid(A)`

```
A = np.dot(X, W) + B
Z = sigmoid(A)
```

---
# 三层神经网络

```
# 输入层
a1 = np.dot(x, W1) + b1
z1 = sigmoid(a1)

# 隐藏层
a2 = np.dot(z1, W2) + b2
z2 = sigmoid(a2)

# 输出层
a3 = np.dot(z2, W3) + b3
y = identity_function(a3)
```

- `identity_function` 为 恒等函数，直接输出值，用于输出层的激活函数
- 注意个层之间的形状，上一层的输出是下一层是输入，所以上一层输出的形状要能够和下一层的权重进行矩阵乘法计算

---
# 输出层设计

机器学习的问题大体上可以分为回归问题和分类问题，一般地，回归问题可以使用恒等函数，
二元分类问题可以使用 sigmoid 函数，多元分类问题可以使用 softmax 函数。

- 在输出层使用恒等函数时，输入信号会原封不动地被输出
- 分类问题中，输出层的神经元的数量设置为要分类的类别数
- softmax函数的输出是 0 到 1 之间的实数, 输出值的总和是 1
	
```
def softmax(a):
   c = np.max(a)
   exp_a = np.exp(a - c) # 溢出对策
   sum_exp_a = np.sum(exp_a)
   y = exp_a / sum_exp_a
   return y

```

`softmax([0.3, 2.9, 4.0]) = [ 0.01821127 0.24519181 0.73659691]`

---
# 推理，前向传播

假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的“推理处理”。
这个推理处理也称为神经网络的前向传播（forward propagation）

- `x, t = get_data()`：加载数据，预处理，x 是输入数据，t 是标签，表示正确答案
- `network = init_network()`：构建网络模型，包括每层的权重 w，偏置 b
- `y = predict(network,  x)`：进行推理，y 表示预测结果，后面会和 t 进行比较

---
# 预处理

- 对神经网络的输入数据进行某种既定的转换称为预处理（pre-processing）
- 将图像的各个像素值除以 255，使得数据的值在 0.0～1.0 的范围内。像这样把数据限定到某个范围内的处理称为正规化（normalization）。
- 利用数据整体的均值或标准差，移动数据，使数据整体以 0 为中心分布，或者进行正规化，把数据的延展控制在一定范围内。
- 除此之外，还有将数据整体的分布形状均匀化的方法，即数据白化（whitening）等

---

# 评估训练结果

使用测试数据对模型进行评估，一条一条的训练，并计算正确率

```
x, t = get_data() # x 是测试数据输入，t 是测试数据结果
network = init_network()
accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    # 获取概率最高的元素的索引，因为 y 是 softmax 的输出
    p = np.argmax(y)
    if p == t[i]:
        accuracy_cnt += 1

# 计算精度
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```

---
# 从数据中学习

所谓“从数据中学习”，是指可以由数据自动决定权重参数的值，比如图片分类，有两种方案：

- 一种方案是，先从图像中人工提取特征量（如 SIFT、SURF和HOG），使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习
- 另一种方案是神经网络，直接学习图像本身，连图像中包含的重要特征量也都是由机器来学习的。

---
# 训练数据和测试数据

- 训练前会把数据分成训练数据集，也叫监督数据（train_data）和测试数据集（test_data）
- 每个数据集会有输入数据（如 x_train，x_test）和标签数据（如 t_train，t_test）两部分，标签数据是人工标准的正确结果
- 批处理：进行训练时经常将数据分成批，每个批次进行训练，而不是每个数据单独训练
    - 这样能够提高性能，减少用在数据传输上的时间，更多时间花在计算上。

---
# 泛化能力，过拟合

- 泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。
- 只对某个数据集过度拟合的状态称为过拟合（over fitting）。避免过拟合也是机器学习的一个重要课题。
- 过拟合意味着泛化能力弱，过拟合一般是因为训练数据太少，或训练的参数太多，模型太复杂造成的。

---
# 损失函数

- 神经网络的学习需要以一个指标为基准，寻找最优权重参数，这个指标可以是任意函数，叫损失函数，通常是均方误差和交叉熵误差等
- 损失函数表示当前的神经网络对监督数据在多大程度上不拟合，取个负值就表示对数据有多大程度上的拟合，也叫性能有多好。
- 有了损失函数后，训练过程就是一个微积分里的一个最优化问题，让损失函数的结果最小化，以求的最优的参数，如权重和偏置等。

---
# 均方误差

均方误差会计算神经网络的输出（y）和正确解监督数据（t）的各个元素之差的平方，再求总和。

```
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

结果值越小，表示 y 和 t 之间的误差越小
---

# 交叉熵

```
def cross_entropy_error(y, t):
    delta = 1e-7 # 防止 y 是 0
    return -np.sum(t * np.log(y + delta))
```

- 交叉熵只计算对应正确解标签的输出的自然对数
- 自然对数函数自变量 x 等于 1 时，因变量 y 为 0；随着 x 向 0 靠近，y 逐渐变小
- 所以正确解标签对应的输出越大，交叉熵函数返回值越接近0
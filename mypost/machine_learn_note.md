# numpy

-  声明数组：`np.array([[1, 2], [3, 4]])`
-  形状：`arr.sharp` = `(2, 2)`
-  类型：`arr.dtype` = `int64`
-  形状相同：`[1, 2] * [3, 4]` = `[3, 8]`
-  形状不同：`[[1, 2], [3, 4]] * [10, 20]` = `[[10, 40], [30, 80]]`
-  数组和数值：`[1, 2] * 10`
-  访问：`arr[[1, 3, 5]]`，`arr[arr < 5]`

---

# matplotlib

```
x = [1, 3, 5]
y = [2, 4, 6]
plt.plot(x, y, label='test')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Hello')
plt.legend()
plt.show()
```

---

# 感知机

概念：接受多个信号，输出一个信号
- 输入信号：x1, x2
- 权重：w1, w2，相当于电阻，控制信号流动的难度
- 输出信号：y
- 阈值：theta，超过阈值神经元被激活，y 为 1

```
w1*x1 + w2*x2 <= theta， y = 0
w1*x1 + w2*x2 > theta, y = 1
```

---
# 逻辑电路：线性可分

- 与门：x1, x2 同时为 1 时 y 为 1
	- w1 = 1, w2 = 1, theta = 1
- 与非门：x1, x2 同时为 0 时 y 为 1
	- w1 = -1, w2 = -1, theta = -1
- 或门：x1, x2 只要有一个为 1 y 就为 1
	- w1 = 1, w2 = 1, theta = 0

三种电路构造形式一样，只有`权重`和`阈值`不同，把构造式子修改下阈值就相当于 `偏置` b

```
b + w1*x1 + w2*x2 <= 0， y = 0
b + w1*x1 + w2*x2 > 0, y = 1
```

这里 `b = -theta`

---
# 与门代码实现

```
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
    return 0
    else:
    return 1
``` 

权重是控制输入信号的重要性，偏置调整神经元被激活的容易程度。

---
# 非线性的异或门，多层感知机

x1 和 x2 只有一个为 1 时 y 为 1，感知机不能表示异或门，因为它是非线性可分的，所以需要多层感知机

```
def XOR(x1, x2):
   s1 = NAND(x1, x2)
   s2 = OR(x1, x2)
   y = AND(s1, s2)
   return y
```

单层感知机只能表示线性空间，而多层感知机可以表示非线性空间

---
# 神经网络，激活函数，阶跃函数

- 输入层，输出层和中间层
- 激活函数：将输入信号的总和转换为输出信号的函数	
- 一旦输入超过阈值，就切换输出，这样的函数称为 `阶跃函数`
- 感知机中使用了阶跃函数作为激活函数，总和大于 0 时返回 1，否则返回 0

---

# sigmoid 函数

```
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

- sigmoid 函数是一条 0 到 1 平滑曲线，阶跃函数是一条 0 到 1 像阶梯一样的折线，两者都属于非线性的函数。
- 神经网络的激活函数必须使用非线性函数，因为使用线性函数的话，加深神经网络的层数就没有意义了

---

# ReLu 函数

ReLU 函数在输入大于 0 时，直接输出该值；在输入小于等于 0 时，输出 0。

```
def relu(x):
    return np.maximum(0, x)
```

---

# 矩阵乘法

- `np.dot`
- 矩阵 A 乘 矩阵 B，A 的第 2 维和 B 的第 1 维的元素个数要一致，结果的形状是 A 的第一维和 B 的第二维
	- 2 * 3 的矩阵 A 乘以 3 * 4 的矩阵 B，结果的形状是 2 * 4
- 二维矩阵和一维数组相乘时，也要保持对应维度的元素个数一致, 结果形状是 A 的第一维长度
	- 2 * 3 的矩阵 A 乘以 3 个元素的数组 a，结果是一个 2 个元素的数组
	- 2 个元素的数组 X 乘以 2 * 3 的矩阵 W 时，结果是一个 3 个元素的数组

---

# 使用矩阵实现 神经网络


- 第一步：`A = X * W + B`
	- X 是输入神经元：`[x1, x2]`
	- W 是权重：`[[w11, w21, w31], [w12, , w22, w32]]`
	- B 是偏置：`[b1, b2, b3]`
	- A 是下一层：`[a1, a2, a3]`
- 第二步：`Z = sigmoid(A)`

```
A = np.dot(X, W) + B
Z = sigmoid(A)
```

---
# 三层神经网络

```
# 输入层
a1 = np.dot(x, W1) + b1
z1 = sigmoid(a1)

# 隐藏层
a2 = np.dot(z1, W2) + b2
z2 = sigmoid(a2)

# 输出层
a3 = np.dot(z2, W3) + b3
y = identity_function(a3)
```

- `identity_function` 为 恒等函数，直接输出值，用于输出层的激活函数
- 注意个层之间的形状，上一层的输出是下一层是输入，所以上一层输出的形状要能够和下一层的权重进行矩阵乘法计算

---
# 输出层设计

一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数，多元分类问题可以使用 softmax 函数。

- 在输出层使用恒等函数时，输入信号会原封不动地被输出
- softmax函数的输出是 0 到 1 之间的实数,输出值的总和是 1
	
```
def softmax(a):
   c = np.max(a)
   exp_a = np.exp(a - c) # 溢出对策
   sum_exp_a = np.sum(exp_a)
   y = exp_a / sum_exp_a
   return y

```

`softmax([0.3, 2.9, 4.0]) = [ 0.01821127 0.24519181 0.73659691]`